<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>data science on Cortically Magnified</title>
    <link>https://www.hhyu.org/tags/data-science/</link>
    <description>Recent content in data science on Cortically Magnified</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 21 Feb 2024 19:42:23 +1100</lastBuildDate><atom:link href="https://www.hhyu.org/tags/data-science/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>How extreme is it? An introduction to extreme statistics</title>
      <link>https://www.hhyu.org/posts/extreme_example/</link>
      <pubDate>Wed, 21 Feb 2024 19:42:23 +1100</pubDate>
      
      <guid>https://www.hhyu.org/posts/extreme_example/</guid>
      <description>It recent years, it has became very obvious that extreme weather is a clear and present danger. On a personal level, just last week, my house in Melbourne was without power for more than 8 hours, due to a large-scale blackout in Victoria caused by strong winds and high temperatures. More shockingly, Two days ago, Carnarvon in Western Australia experienced a 49.9Â°C day, which is said to be the world&amp;rsquo;s hottest temperature recorded this year.</description>
    </item>
    
    <item>
      <title>What can we learn from the Simpson&#39;s Paradox?</title>
      <link>https://www.hhyu.org/posts/simpsons/</link>
      <pubDate>Fri, 28 Jan 2022 17:06:28 +1100</pubDate>
      
      <guid>https://www.hhyu.org/posts/simpsons/</guid>
      <description>The Simpson&amp;rsquo;s Paradox is one of the most well-known paradoxes in statistics. A quick google will find plenty of blog posts (many from the data science community) about this puzzling phenomenon. It is clearly a topic of real-world significance. There seem to be some important lessons that we are supposed to learn from it. But what are those lessons? Is it nothing more than a cautionary tale about how easy it is for data analyses to go wrong?</description>
    </item>
    
    <item>
      <title>A mind-boggling analogy between machine learning and quantum physics</title>
      <link>https://www.hhyu.org/posts/fermion/</link>
      <pubDate>Fri, 27 Aug 2021 20:47:54 +1000</pubDate>
      
      <guid>https://www.hhyu.org/posts/fermion/</guid>
      <description>A recent paper published in PNAS titled &amp;ldquo;The Fermi-Dirac distribution provides a calibrated probabilistic output for binary classifiers&amp;rdquo; caught my attention, because it describes a surprising relationship between machine learning and quantum physics. In fact, surprising is an understatement. Mind-boggling is more like it. According to the analogy developed by the authors, positive samples in binary classification problems are like&amp;hellip; fermions?! What?! I decided that I should try to understand the gist of this paper, at least to the extent that I can.</description>
    </item>
    
    <item>
      <title>Use basic data science skills to debunk a myth about koalas!</title>
      <link>https://www.hhyu.org/posts/koala/</link>
      <pubDate>Fri, 05 Feb 2021 20:20:25 +1100</pubDate>
      
      <guid>https://www.hhyu.org/posts/koala/</guid>
      <description>Did you know that the koala is the dumbest animal in the world? According to an Internet meme, koalas have really tiny brains because the eucalyptus leaves that they eat are toxic and poor in nutrition. That seems plausible to me, but you shouldn&amp;rsquo;t believe in Internet memes. Let&amp;rsquo;s turn to the most authoritative source of knowledge in the world, the Wikipedia, instead. This is what the Wikipedia has to say about koala&amp;rsquo;s brain:</description>
    </item>
    
  </channel>
</rss>
